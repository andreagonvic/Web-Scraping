{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ¡IMPORTANTE! Leer primero el apartado de requisitos para descargar todo lo necesario para ejecutar el programa\n",
    "\n",
    "# Importamos todas las librerías necesarias\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import selenium \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "\n",
    "# Pasamos como argumento la web pricipal que scrapeamos\n",
    "url = \"https://es.investing.com/equities/most-active-stocks\"\n",
    "\n",
    "# Y la ruta en la que hemos guardado la extensión previamente descargada \"ChromeDriver\"\n",
    "PATH = \"/Applications/chromedriver\"\n",
    "\n",
    "def most_active_stocks(url, PATH):\n",
    "     \n",
    "    #Para descargar el contenido de la pagina\n",
    "    \n",
    "    #url = \"https://es.investing.com/equities/\"stock\"-historical-data\"\n",
    "    \n",
    "    r=requests.get(url)\n",
    "\n",
    "    #Creamos el árbol de objetos Python que representan al documento HTML (la sopa)\n",
    "    soup = BeautifulSoup(r.text, 'lxml')\n",
    "    \n",
    "    #Tenemos que \"engañar\" a la pagina, para que crea que somos un navegador, por ello, incluimos headers, sino, obtendriamos el siguiente error:\n",
    "    #<title>403 You are banned from this site.  Please contact via a different client configuration if you believe that this is a mistake.</title>\n",
    "\n",
    "    header={'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.0 Safari/537.36'}\n",
    "    page=requests.get(url,headers=header)\n",
    "\n",
    "    # Creamos las listas y el diccionario para recoger el data set\n",
    "    list_name = []\n",
    "    list_last = []\n",
    "    list_high = []\n",
    "    list_vol = []\n",
    "    list_low = []\n",
    "    list_time = []\n",
    "    list_var = []\n",
    "    list_por_var = []\n",
    "    list_web = []\n",
    "    dict_data = {}\n",
    "    \n",
    "    # Comprobamos que la petición nos devuelve un Status Code = 200\n",
    "    status_code = page.status_code\n",
    "    if status_code == 200:\n",
    "        \n",
    "        soup=BeautifulSoup(page.content,'html.parser')\n",
    "        result = soup.find('td',attrs={'class':'left bold plusIconTd elp'})\n",
    "    \n",
    "        # Crearemos un bucle que vaya recogiendo los datos y los vaya metiendo a sus correspondientes listas\n",
    "        for link in soup.find_all('td',attrs={'class':'left bold plusIconTd elp'}):\n",
    "            name= link.get_text()\n",
    "            list_name.append(name)\n",
    "            \n",
    "            web_name = re.findall('\"/equities/(.*)\" title',str(link))[0]\n",
    "            list_web.append(web_name)\n",
    "            \n",
    "            line = link.find('span',{'class':'alertBellGrayPlus js-plus-icon genToolTip oneliner'})\n",
    "            id_valor = re.findall('\\D(\\d{3,7})\\D', str(line))[0]\n",
    "            \n",
    "            \n",
    "            new_class_1 = \"align_right pid-\"+ id_valor +\"-last\"\n",
    "            last=soup.find('td',attrs={'class':new_class_1}).getText()\n",
    "            list_last.append(last)\n",
    "\n",
    "            new_class_2 = \"pid-\"+ id_valor +\"-time\"\n",
    "            time=soup.find('td',attrs={'class':new_class_2}).getText()\n",
    "            list_time.append(time)\n",
    "\n",
    "            new_class_3 = \"pid-\"+ id_valor +\"-turnover\"\n",
    "            vol=soup.find('td',attrs={'class':new_class_3}).getText()\n",
    "            list_vol.append(vol)\n",
    "\n",
    "            new_class_4 = \"align_right pid-\"+ id_valor+\"-high\"\n",
    "            high=soup.find('td',attrs={'class':new_class_4}).getText()\n",
    "            list_high.append(high)\n",
    "\n",
    "            new_class_5 = \"pid-\"+ id_valor +\"-low\"\n",
    "            low=soup.find('td',attrs={'class':new_class_5}).getText()\n",
    "            list_low.append(low)\n",
    "\n",
    "            if soup.find('td',attrs={'class':\"bold redFont pid-\"+id_valor+\"-pc\"}) != None:\n",
    "                var = soup.find('td',attrs={'class':\"bold redFont pid-\"+id_valor+\"-pc\"}).getText()\n",
    "                por_var = soup.find('td',attrs={'class':\"bold redFont pid-\"+id_valor+\"-pcp\"}).getText()\n",
    "\n",
    "            elif soup.find('td',attrs={'class':\"bold greenFont pid-\"+id_valor+\"-pc\"}) != None:\n",
    "                var = soup.find('td',attrs={'class':\"bold greenFont pid-\"+id_valor+\"-pc\"}).getText()\n",
    "                por_var = soup.find('td',attrs={'class':\"bold greenFont pid-\"+id_valor+\"-pcp\"}).getText()\n",
    "\n",
    "            else:\n",
    "                var = soup.find('td',attrs={'class':\"bold blackFont pid-\"+id_valor+\"-pc\"}).getText()\n",
    "                por_var = soup.find('td',attrs={'class':\"bold blackFont pid-\"+id_valor+\"-pcp\"}).getText()\n",
    "\n",
    "            list_var.append(var)\n",
    "            list_por_var.append(por_var)\n",
    "            \n",
    "            \n",
    "        for web_name in list_web:\n",
    "            #Para descargar el contenido de la pagina\n",
    "            \n",
    "            if web_name == \"intl.-cons.-air-grp?cid=13809\":\n",
    "                url_2 = \"https://es.investing.com/equities/intl.-cons.-air-grp-historical-data?cid=13809\"\n",
    "            elif web_name == \"arcelormittal-reg?cid=32439\":\n",
    "                url_2 = \"https://es.investing.com/equities/arcelormittal-reg-historical-data?cid=32439\"\n",
    "            elif web_name == \"eads?cid=32302\":\n",
    "                url_2 = \"https://es.investing.com/equities/eads-historical-data?cid=32302\"\n",
    "            else:\n",
    "                url_2 = \"https://es.investing.com/equities/\" + web_name + \"-historical-data\"\n",
    "    \n",
    "            r=requests.get(url_2)\n",
    "\n",
    "            #Creamos el árbol de objetos Python que representan al documento HTML (la sopa)\n",
    "            soup = BeautifulSoup(r.text, 'lxml')\n",
    "\n",
    "            #Tenemos que \"engañar\" a la pagina, para que crea que somos un navegador, por ello, incluimos headers, sino, obtendriamos el siguiente error:\n",
    "            #<title>403 You are banned from this site.  Please contact via a different client configuration if you believe that this is a mistake.</title>\n",
    "\n",
    "            header={'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2227.0 Safari/537.36'}\n",
    "            page=requests.get(url_2,headers=header)\n",
    "            \n",
    "            # Comprobamos que la petición nos devuelve un Status Code = 200\n",
    "            status_code = page.status_code\n",
    "            if status_code == 200:\n",
    "\n",
    "                # Se selciona la extension descargada y con ella se ejecuta atomáticamente nuestra  búsqueda\n",
    "                driver = webdriver.Chrome(PATH)\n",
    "                driver.get(url_2)\n",
    "                \n",
    "                # Elegimos el desplgable y la opción que queremos mostrar\n",
    "                select = Select(driver.find_element_by_id('data_interval'))\n",
    "                select.select_by_visible_text('Mensual')\n",
    "                \n",
    "                soup=BeautifulSoup(page.content,'html.parser')\n",
    "                \n",
    "                # Creamos las listas y el diccionario para recoger el data set\n",
    "                list_date_hist = []\n",
    "                list_price_hist = []\n",
    "                list_open_hist = []\n",
    "                list_high_hist = []\n",
    "                list_low_hist = []\n",
    "                list_vol_hist = []\n",
    "                list_var_hist = []\n",
    "                dict_data_hist = {}\n",
    "                \n",
    "                # Crearemos un bucle que vaya recogiendo los datos y los vaya metiendo a sus correspondientes listas\n",
    "                for dato in driver.find_elements_by_xpath('//*[@id=\"curr_table\"]'):\n",
    "                    data_equities = dato.text\n",
    "                    \n",
    "                    # Se recoge toda la información en un mismo elemento que debemos ir dividiendo y clasificando en las listas\n",
    "                    data_by_month = data_equities.split('\\n')\n",
    "                    \n",
    "                    indice = 1\n",
    "                    conuter_year = 0\n",
    "                    \n",
    "                    for i in data_by_month: \n",
    "                        data_by_value = data_by_month[indice].split()\n",
    "                        indice += 1\n",
    "                        \n",
    "                        if indice == len(data_by_month):\n",
    "                            break\n",
    "                        \n",
    "                        counter = 0    \n",
    "                        \n",
    "                        for j in data_by_value:  \n",
    "                            if counter == 0: \n",
    "                                if conuter_year < 11:\n",
    "                                    list_date_hist.append(j + \" 2020\")\n",
    "                                else:\n",
    "                                    list_date_hist.append(j + \" 2019\")\n",
    "                            elif counter == 2:\n",
    "                                list_price_hist.append(j)\n",
    "                            elif counter == 3:\n",
    "                                list_open_hist.append(j)\n",
    "                            elif counter == 4:\n",
    "                                list_high_hist.append(j)\n",
    "                            elif counter == 5:\n",
    "                                list_low_hist.append(j)\n",
    "                            elif counter == 6:\n",
    "                                list_vol_hist.append(j)\n",
    "                            elif counter == 7:\n",
    "                                list_var_hist.append(j)                          \n",
    "                            counter += 1 \n",
    "                            \n",
    "                            if counter == len(data_by_value):\n",
    "                                break\n",
    "                        \n",
    "                        conuter_year += 1\n",
    "                \n",
    "                # Creamos las entreadas del diccionario y lo convertimos en un dataframe    \n",
    "                dict_data_hist[\"Fecha\"] = list_date_hist\n",
    "                dict_data_hist[\"Último valor\"] = list_price_hist\n",
    "                dict_data_hist[\"Valor apertura\"] = list_open_hist\n",
    "                dict_data_hist[\"Máx. valor\"] = list_high_hist\n",
    "                dict_data_hist[\"Min. valor\"] = list_low_hist\n",
    "                dict_data_hist[\"Volumen\"] = list_vol_hist\n",
    "                dict_data_hist[\"% Variación\"] = list_var_hist\n",
    "                \n",
    "                df_hist = pd.DataFrame(dict_data_hist, columns = ['Fecha', 'Último valor', 'Valor apertura', 'Máx. valor', 'Min. valor', 'Volumen', '% Variación'])\n",
    "\n",
    "                # Descargamos un archivo .csv \n",
    "                final_datafile_hist = df_hist.to_csv('WebScraping_SP_stock_'+ web_name + '.csv', sep=',')\n",
    "                    \n",
    "            else:\n",
    "                print(\"Status Code %d\" % status_code)\n",
    "    \n",
    "        # Creamos las entreadas del diccionario y lo convertimos en un dataframe\n",
    "        dict_data['Nombre'] = list_name\n",
    "        dict_data['Último valor'] = list_last\n",
    "        dict_data['Máx. valor'] = list_high\n",
    "        dict_data['Min. valor'] = list_low\n",
    "        dict_data['Variación'] = list_var\n",
    "        dict_data['% Variación'] = list_por_var\n",
    "        dict_data['Volumen'] = list_vol\n",
    "        dict_data['Hora'] = list_time\n",
    "\n",
    "        df = pd.DataFrame(dict_data, columns = ['Nombre', 'Último valor', 'Máx. valor', 'Min. valor', 'Variación', '% Variación', 'Volumen', 'Hora'])\n",
    "\n",
    "        # Descargamos un archivo .csv \n",
    "        final_datafile = df.to_csv('WebScraping_SP_stocks.csv', sep=',')\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        print(\"Status Code %d\" % status_code)\n",
    "        \n",
    "    return\n",
    "\n",
    "# Ejecutamos la función creada\n",
    "\n",
    "most_active_stocks(url, PATH)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
